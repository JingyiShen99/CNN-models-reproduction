{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afd14f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b8567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "picProcessor = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean = [0.1 / 1.275],\n",
    "        std = [1.0 / 1.275]\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f744c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"E:\\\\github\\\\CNN-models-reproduction\\\\LeNet\\\\dataset\\\\\" \n",
    "mnistTrain = datasets.MNIST(dataPath, train = True,  download = False, transform = picProcessor)\n",
    "mnistTest = datasets.MNIST(dataPath, train = False, download = False, transform = picProcessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb67c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b924a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subsampling(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super(Subsampling,self).__init__()\n",
    "        self.pool = nn.AvgPool2d(2)\n",
    "        self.in_channel = in_channel\n",
    "        F_in = 4 * self.in_channel\n",
    "        self.weight = nn.Parameter(torch.rand(self.in_channel) * 4.8 / F_in - 2.4 / F_in, requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.rand(self.in_channel), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        outs = [] #对每一个channel的特征图进行池化，结果存储在这里\n",
    "\n",
    "        for channel in range(self.in_channel):\n",
    "            out = x[:, channel] * self.weight[channel] + self.bias[channel] #这一步计算每一个channel的池化结果[batch_size, height, weight]\n",
    "            outs.append(out.unsqueeze(1)) #把channel的维度加进去[batch_size, channel, height, weight]\n",
    "        return torch.cat(outs, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "405b3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapConv(nn.Module):\n",
    "    def __init__(self,in_channel, out_channel, kernel_size = 5):\n",
    "        super(MapConv,self).__init__()\n",
    "        mapInfo = [[1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1],\n",
    "                   [1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1],\n",
    "                   [1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1],\n",
    "                   [0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1],\n",
    "                   [0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1],\n",
    "                   [0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1]]\n",
    "        mapInfo = torch.tensor(mapInfo, dtype = torch.long)\n",
    "        self.register_buffer(\"mapInfo\", mapInfo) #在Module中的buffer中的参数是不会被求梯度的\n",
    "        \n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        \n",
    "        self.convs = {} #将每一个定义的卷积层都放进这个字典\n",
    "        \n",
    "        #对每一个新建立的卷积层都进行注册，使其真正成为模块并且方便调用\n",
    "        for i in range(self.out_channel):\n",
    "            conv = nn.Conv2d(mapInfo[:, i].sum().item(), 1, kernel_size)\n",
    "            convName = \"conv{}\".format(i)\n",
    "            self.convs[convName] = conv\n",
    "            self.add_module(convName, conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = [] #对每一个卷积层通过映射来计算卷积，结果存储在这里\n",
    "        \n",
    "        for i in range(self.out_channel):\n",
    "            mapIdx = self.mapInfo[:, i].nonzero().squeeze()\n",
    "            convInput = x.index_select(1, mapIdx)\n",
    "            convOutput = self.convs['conv{}'.format(i)](convInput)\n",
    "            outs.append(convOutput)\n",
    "        return torch.cat(outs, dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "866cd2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_zero = [-1, +1, +1, +1, +1, +1, -1] + \\\n",
    "        [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "        [-1, -1, +1, +1, +1, -1, -1] + \\\n",
    "        [-1, +1, +1, -1, +1, +1, -1] + \\\n",
    "        [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "        [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "        [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "        [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "        [-1, +1, +1, -1, +1, +1, -1] + \\\n",
    "        [-1, -1, +1, +1, +1, -1, -1] + \\\n",
    "        [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "        [-1, -1, -1, -1, -1, -1, -1]\n",
    "\n",
    "_one = [-1, -1, -1, +1, +1, -1, -1] + \\\n",
    "       [-1, -1, +1, +1, +1, -1, -1] + \\\n",
    "       [-1, +1, +1, +1, +1, -1, -1] + \\\n",
    "       [-1, -1, -1, +1, +1, -1, -1] + \\\n",
    "       [-1, -1, -1, +1, +1, -1, -1] + \\\n",
    "       [-1, -1, -1, +1, +1, -1, -1] + \\\n",
    "       [-1, -1, -1, +1, +1, -1, -1] + \\\n",
    "       [-1, -1, -1, +1, +1, -1, -1] + \\\n",
    "       [-1, -1, -1, +1, +1, -1, -1] + \\\n",
    "       [-1, +1, +1, +1, +1, +1, +1] + \\\n",
    "       [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "       [-1, -1, -1, -1, -1, -1, -1]\n",
    "\n",
    "_two = [-1, +1, +1, +1, +1, +1, -1] + \\\n",
    "       [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "       [-1, +1, +1, +1, +1, +1, -1] + \\\n",
    "       [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "       [+1, -1, -1, -1, -1, +1, +1] + \\\n",
    "       [-1, -1, -1, -1, +1, +1, -1] + \\\n",
    "       [-1, -1, +1, +1, +1, -1, -1] + \\\n",
    "       [-1, +1, +1, -1, -1, -1, -1] + \\\n",
    "       [+1, +1, -1, -1, -1, -1, -1] + \\\n",
    "       [+1, +1, +1, +1, +1, +1, +1] + \\\n",
    "       [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "       [-1, -1, -1, -1, -1, -1, -1]\n",
    "\n",
    "_three = [+1, +1, +1, +1, +1, +1, +1] + \\\n",
    "         [-1, -1, -1, -1, -1, +1, +1] + \\\n",
    "         [-1, -1, -1, -1, +1, +1, -1] + \\\n",
    "         [-1, -1, -1, +1, +1, -1, -1] + \\\n",
    "         [-1, -1, +1, +1, +1, +1, -1] + \\\n",
    "         [-1, -1, -1, -1, -1, +1, +1] + \\\n",
    "         [-1, -1, -1, -1, -1, +1, +1] + \\\n",
    "         [-1, -1, -1, -1, -1, +1, +1] + \\\n",
    "         [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "         [-1, +1, +1, +1, +1, +1, -1] + \\\n",
    "         [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "         [-1, -1, -1, -1, -1, -1, -1]\n",
    "\n",
    "_four = [-1, +1, +1, +1, +1, +1, -1] + \\\n",
    "        [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "        [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "        [-1, +1, +1, -1, -1, +1, +1] + \\\n",
    "        [-1, +1, +1, -1, -1, +1, +1] + \\\n",
    "        [+1, +1, +1, -1, -1, +1, +1] + \\\n",
    "        [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "        [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "        [+1, +1, -1, -1, +1, +1, +1] + \\\n",
    "        [-1, +1, +1, +1, +1, +1, +1] + \\\n",
    "        [-1, -1, -1, -1, -1, +1, +1] + \\\n",
    "        [-1, -1, -1, -1, -1, +1, +1]\n",
    "\n",
    "_five = [-1, +1, +1, +1, +1, +1, -1] + \\\n",
    "        [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "        [+1, +1, +1, +1, +1, +1, +1] + \\\n",
    "        [+1, +1, -1, -1, -1, -1, -1] + \\\n",
    "        [+1, +1, -1, -1, -1, -1, -1] + \\\n",
    "        [-1, +1, +1, +1, +1, -1, -1] + \\\n",
    "        [-1, -1, +1, +1, +1, +1, -1] + \\\n",
    "        [-1, -1, -1, -1, -1, +1, +1] + \\\n",
    "        [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "        [-1, +1, +1, +1, +1, +1, -1] + \\\n",
    "        [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "        [-1, -1, -1, -1, -1, -1, -1]\n",
    "\n",
    "_six = [-1, -1, +1, +1, +1, +1, -1] + \\\n",
    "       [-1, +1, +1, -1, -1, -1, -1] + \\\n",
    "       [+1, +1, -1, -1, -1, -1, -1] + \\\n",
    "       [+1, +1, -1, -1, -1, -1, -1] + \\\n",
    "       [+1, +1, +1, +1, +1, +1, -1] + \\\n",
    "       [+1, +1, +1, -1, -1, +1, +1] + \\\n",
    "       [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "       [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "       [+1, +1, +1, -1, -1, +1, +1] + \\\n",
    "       [-1, +1, +1, +1, +1, +1, -1] + \\\n",
    "       [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "       [-1, -1, -1, -1, -1, -1, -1]\n",
    "\n",
    "_seven = [+1, +1, +1, +1, +1, +1, +1] + \\\n",
    "         [-1, -1, -1, -1, -1, +1, +1] + \\\n",
    "         [-1, -1, -1, -1, -1, +1, +1] + \\\n",
    "         [-1, -1, -1, -1, +1, +1, -1] + \\\n",
    "         [-1, -1, -1, +1, +1, -1, -1] + \\\n",
    "         [-1, -1, -1, +1, +1, -1, -1] + \\\n",
    "         [-1, -1, +1, +1, -1, -1, -1] + \\\n",
    "         [-1, -1, +1, +1, -1, -1, -1] + \\\n",
    "         [-1, -1, +1, +1, -1, -1, -1] + \\\n",
    "         [-1, -1, +1, +1, -1, -1, -1] + \\\n",
    "         [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "         [-1, -1, -1, -1, -1, -1, -1]\n",
    "\n",
    "_eight = [-1, +1, +1, +1, +1, +1, -1] + \\\n",
    "         [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "         [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "         [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "         [-1, +1, +1, +1, +1, +1, -1] + \\\n",
    "         [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "         [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "         [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "         [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "         [-1, +1, +1, +1, +1, +1, -1] + \\\n",
    "         [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "         [-1, -1, -1, -1, -1, -1, -1]\n",
    "\n",
    "_nine = [-1, +1, +1, +1, +1, +1, -1] + \\\n",
    "        [+1, +1, -1, -1, +1, +1, +1] + \\\n",
    "        [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "        [+1, +1, -1, -1, -1, +1, +1] + \\\n",
    "        [+1, +1, -1, -1, +1, +1, +1] + \\\n",
    "        [-1, +1, +1, +1, +1, +1, +1] + \\\n",
    "        [-1, -1, -1, -1, -1, +1, +1] + \\\n",
    "        [-1, -1, -1, -1, -1, +1, +1] + \\\n",
    "        [-1, -1, -1, -1, +1, +1, -1] + \\\n",
    "        [-1, +1, +1, +1, +1, -1, -1] + \\\n",
    "        [-1, -1, -1, -1, -1, -1, -1] + \\\n",
    "        [-1, -1, -1, -1, -1, -1, -1]\n",
    "\n",
    "\n",
    "RBF_WEIGHT = np.array([_zero, _one, _two, _three, _four, _five, _six, _seven, _eight, _nine]).transpose()\n",
    "\n",
    "class RBFLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, init_weight = None):\n",
    "        super(RBFLayer, self).__init__()\n",
    "        if init_weight is not None:\n",
    "            self.register_buffer(\"weight\", torch.tensor(init_weight))\n",
    "        else:\n",
    "            self.register_buffer(\"weight\", torch.rand(in_features, out_features))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = (x - self.weight).pow(2).sum(-2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e78c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.C1 = nn.Conv2d(1, 6, 5, padding = 2, padding_mode = 'replicate')\n",
    "        self.S2 = Subsampling(6)\n",
    "        self.C3 = MapConv(6, 16, 5)\n",
    "        self.S4 = Subsampling(16)\n",
    "        self.C5 = nn.Conv2d(16, 120, 5)\n",
    "        self.F6 = nn.Linear(120, 84)\n",
    "        self.Output = RBFLayer(84, 10, RBF_WEIGHT)\n",
    "        \n",
    "        self.act = nn.Tanh()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                F_in = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "                m.weight.data = torch.rand(m.weight.data.size()) * 4.8 / F_in - 2.4 / F_in\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                F_in = m.in_features\n",
    "                m.weight.data = torch.rand(m.weight.data.size()) * 4.8 / F_in - 2.4 / F_in\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.C1(x)\n",
    "        x = 1.7159 * self.act(2 * self.S2(x) / 3)\n",
    "        x = self.C3(x)\n",
    "        x = 1.7159 * self.act(2 * self.S4(x) / 3)\n",
    "        x = self.C5(x)\n",
    "        \n",
    "        x = x.view(-1, 120)\n",
    "        \n",
    "        x = 1.7159 * self.act(2 * self.F6(x) / 3)\n",
    "        \n",
    "        out = self.Output(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13a5e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, label):\n",
    "    if(label.dim() == 1):\n",
    "        return pred[torch.arange(pred.size(0)), label]\n",
    "    else:\n",
    "        return pred[torch.arange(pred.size(0)), label.squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88a0a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossList = []\n",
    "testError = []\n",
    "trainError = []\n",
    "def train(epochs, model, optimizer, scheduler: bool, loss_fn, trainSet, testSet):\n",
    "\n",
    "    trainNum = len(trainSet)\n",
    "    testNum = len(testSet)\n",
    "    for epoch in range(epochs):\n",
    "        lossSum = 0.0\n",
    "        print(\"epoch: {:02d} / {:d}\".format(epoch+1, epochs))\n",
    "        \n",
    "        for idx, (img, label) in enumerate(trainSet):\n",
    "            \n",
    "            x = img.unsqueeze(0).to(device)\n",
    "            y = torch.tensor([label], dtype = torch.long).to(device)\n",
    "            \n",
    "            out = model(x)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            lossSum += loss.item()\n",
    "            if (idx + 1) % 2000 == 0: print(\"sample: {:05d} / {:d} --> loss: {:.4f}\".format(idx+1, trainNum, loss.item()))\n",
    "        \n",
    "        lossList.append(lossSum / trainNum)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            errorNum = 0\n",
    "            for img, label in trainSet:\n",
    "                x = img.unsqueeze(0).to(device)\n",
    "                out = model(x)\n",
    "                _, pred_y = out.min(dim = 1)\n",
    "                if(pred_y != label): errorNum += 1\n",
    "            trainError.append(errorNum / trainNum)\n",
    "            \n",
    "            errorNum = 0\n",
    "            for img, label in testSet:\n",
    "                x = img.unsqueeze(0).to(device)\n",
    "                out = model(x)\n",
    "                _, pred_y = out.min(dim = 1)\n",
    "                if(pred_y != label): errorNum += 1\n",
    "            testError.append(errorNum / testNum)\n",
    "        \n",
    "        if scheduler == True:\n",
    "            if epoch < 5:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 1.0e-3\n",
    "            elif epoch < 10:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 5.0e-4\n",
    "            elif epoch < 15:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 2.0e-4\n",
    "            else:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 1.0e-4\n",
    "\n",
    "    torch.save(model.state_dict(), 'E:\\\\github\\\\CNN-models-reproduction\\\\LeNet\\\\5epoch-{:d}_loss-{:.6f}_error-{:.2%}.pth'.format(epochs, lossList[-1], testError[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "301d5181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01 / 25\n",
      "sample: 02000 / 60000 --> loss: 50.4780\n",
      "sample: 04000 / 60000 --> loss: 45.3264\n",
      "sample: 06000 / 60000 --> loss: 50.1763\n",
      "sample: 08000 / 60000 --> loss: 61.2208\n",
      "sample: 10000 / 60000 --> loss: 75.8370\n",
      "sample: 12000 / 60000 --> loss: 48.7051\n",
      "sample: 14000 / 60000 --> loss: 47.3273\n",
      "sample: 16000 / 60000 --> loss: 79.6415\n",
      "sample: 18000 / 60000 --> loss: 44.4480\n",
      "sample: 20000 / 60000 --> loss: 57.5230\n",
      "sample: 22000 / 60000 --> loss: 55.7501\n",
      "sample: 24000 / 60000 --> loss: 61.5074\n",
      "sample: 26000 / 60000 --> loss: 49.0858\n",
      "sample: 28000 / 60000 --> loss: 45.4849\n",
      "sample: 30000 / 60000 --> loss: 84.0638\n",
      "sample: 32000 / 60000 --> loss: 46.5164\n",
      "sample: 34000 / 60000 --> loss: 50.3390\n",
      "sample: 36000 / 60000 --> loss: 48.9910\n",
      "sample: 38000 / 60000 --> loss: 74.4047\n",
      "sample: 40000 / 60000 --> loss: 46.4023\n",
      "sample: 42000 / 60000 --> loss: 50.9585\n",
      "sample: 44000 / 60000 --> loss: 48.9884\n",
      "sample: 46000 / 60000 --> loss: 82.8445\n",
      "sample: 48000 / 60000 --> loss: 53.5194\n",
      "sample: 50000 / 60000 --> loss: 46.5151\n",
      "sample: 52000 / 60000 --> loss: 50.9368\n",
      "sample: 54000 / 60000 --> loss: 43.8929\n",
      "sample: 56000 / 60000 --> loss: 45.4490\n",
      "sample: 58000 / 60000 --> loss: 53.5083\n",
      "sample: 60000 / 60000 --> loss: 44.1203\n",
      "epoch: 02 / 25\n",
      "sample: 02000 / 60000 --> loss: 49.7481\n",
      "sample: 04000 / 60000 --> loss: 46.6888\n",
      "sample: 06000 / 60000 --> loss: 49.5931\n",
      "sample: 08000 / 60000 --> loss: 61.4835\n",
      "sample: 10000 / 60000 --> loss: 74.4862\n",
      "sample: 12000 / 60000 --> loss: 47.9082\n",
      "sample: 14000 / 60000 --> loss: 47.6771\n",
      "sample: 16000 / 60000 --> loss: 80.5637\n",
      "sample: 18000 / 60000 --> loss: 44.5729\n",
      "sample: 20000 / 60000 --> loss: 57.2485\n",
      "sample: 22000 / 60000 --> loss: 56.7518\n",
      "sample: 24000 / 60000 --> loss: 60.7854\n",
      "sample: 26000 / 60000 --> loss: 48.9587\n",
      "sample: 28000 / 60000 --> loss: 45.7491\n",
      "sample: 30000 / 60000 --> loss: 84.1768\n",
      "sample: 32000 / 60000 --> loss: 46.6445\n",
      "sample: 34000 / 60000 --> loss: 50.2592\n",
      "sample: 36000 / 60000 --> loss: 49.2532\n",
      "sample: 38000 / 60000 --> loss: 74.8416\n",
      "sample: 40000 / 60000 --> loss: 53.4405\n",
      "sample: 42000 / 60000 --> loss: 22.7182\n",
      "sample: 44000 / 60000 --> loss: 33.9982\n",
      "sample: 46000 / 60000 --> loss: 3.1350\n",
      "sample: 48000 / 60000 --> loss: 23.0207\n",
      "sample: 50000 / 60000 --> loss: 52.7004\n",
      "sample: 52000 / 60000 --> loss: 31.2415\n",
      "sample: 54000 / 60000 --> loss: 38.5836\n",
      "sample: 56000 / 60000 --> loss: 2.6441\n",
      "sample: 58000 / 60000 --> loss: 7.7397\n",
      "sample: 60000 / 60000 --> loss: 20.9947\n",
      "epoch: 03 / 25\n",
      "sample: 02000 / 60000 --> loss: 10.0757\n",
      "sample: 04000 / 60000 --> loss: 7.0432\n",
      "sample: 06000 / 60000 --> loss: 3.5755\n",
      "sample: 08000 / 60000 --> loss: 3.3985\n",
      "sample: 10000 / 60000 --> loss: 11.2462\n",
      "sample: 12000 / 60000 --> loss: 13.2836\n",
      "sample: 14000 / 60000 --> loss: 3.8669\n",
      "sample: 16000 / 60000 --> loss: 6.7243\n",
      "sample: 18000 / 60000 --> loss: 9.2925\n",
      "sample: 20000 / 60000 --> loss: 4.8483\n",
      "sample: 22000 / 60000 --> loss: 1.6653\n",
      "sample: 24000 / 60000 --> loss: 5.3689\n",
      "sample: 26000 / 60000 --> loss: 3.0032\n",
      "sample: 28000 / 60000 --> loss: 6.8068\n",
      "sample: 30000 / 60000 --> loss: 11.1981\n",
      "sample: 32000 / 60000 --> loss: 5.0098\n",
      "sample: 34000 / 60000 --> loss: 14.5498\n",
      "sample: 36000 / 60000 --> loss: 3.4000\n",
      "sample: 38000 / 60000 --> loss: 3.0354\n",
      "sample: 40000 / 60000 --> loss: 2.3784\n",
      "sample: 42000 / 60000 --> loss: 21.6609\n",
      "sample: 44000 / 60000 --> loss: 5.0707\n",
      "sample: 46000 / 60000 --> loss: 4.2359\n",
      "sample: 48000 / 60000 --> loss: 1.8428\n",
      "sample: 50000 / 60000 --> loss: 8.8685\n",
      "sample: 52000 / 60000 --> loss: 6.5628\n",
      "sample: 54000 / 60000 --> loss: 33.8988\n",
      "sample: 56000 / 60000 --> loss: 4.0485\n",
      "sample: 58000 / 60000 --> loss: 6.8714\n",
      "sample: 60000 / 60000 --> loss: 16.6034\n",
      "epoch: 04 / 25\n",
      "sample: 02000 / 60000 --> loss: 12.0972\n",
      "sample: 04000 / 60000 --> loss: 6.3071\n",
      "sample: 06000 / 60000 --> loss: 3.3145\n",
      "sample: 08000 / 60000 --> loss: 3.8266\n",
      "sample: 10000 / 60000 --> loss: 7.8076\n",
      "sample: 12000 / 60000 --> loss: 19.5154\n",
      "sample: 14000 / 60000 --> loss: 4.6177\n",
      "sample: 16000 / 60000 --> loss: 9.3986\n",
      "sample: 18000 / 60000 --> loss: 7.8257\n",
      "sample: 20000 / 60000 --> loss: 4.9320\n",
      "sample: 22000 / 60000 --> loss: 2.2576\n",
      "sample: 24000 / 60000 --> loss: 3.8309\n",
      "sample: 26000 / 60000 --> loss: 2.6173\n",
      "sample: 28000 / 60000 --> loss: 4.8791\n",
      "sample: 30000 / 60000 --> loss: 10.2634\n",
      "sample: 32000 / 60000 --> loss: 5.0364\n",
      "sample: 34000 / 60000 --> loss: 12.8038\n",
      "sample: 36000 / 60000 --> loss: 4.6729\n",
      "sample: 38000 / 60000 --> loss: 2.9394\n",
      "sample: 40000 / 60000 --> loss: 3.2992\n",
      "sample: 42000 / 60000 --> loss: 34.0812\n",
      "sample: 44000 / 60000 --> loss: 5.2093\n",
      "sample: 46000 / 60000 --> loss: 3.2335\n",
      "sample: 48000 / 60000 --> loss: 2.0824\n",
      "sample: 50000 / 60000 --> loss: 4.3456\n",
      "sample: 52000 / 60000 --> loss: 6.8664\n",
      "sample: 54000 / 60000 --> loss: 26.4754\n",
      "sample: 56000 / 60000 --> loss: 5.4110\n",
      "sample: 58000 / 60000 --> loss: 5.4843\n",
      "sample: 60000 / 60000 --> loss: 10.1254\n",
      "epoch: 05 / 25\n",
      "sample: 02000 / 60000 --> loss: 14.7329\n",
      "sample: 04000 / 60000 --> loss: 13.3629\n",
      "sample: 06000 / 60000 --> loss: 4.2382\n",
      "sample: 08000 / 60000 --> loss: 2.7036\n",
      "sample: 10000 / 60000 --> loss: 4.1457\n",
      "sample: 12000 / 60000 --> loss: 20.6466\n",
      "sample: 14000 / 60000 --> loss: 4.1207\n",
      "sample: 16000 / 60000 --> loss: 7.9557\n",
      "sample: 18000 / 60000 --> loss: 6.0014\n",
      "sample: 20000 / 60000 --> loss: 4.5607\n",
      "sample: 22000 / 60000 --> loss: 1.9019\n",
      "sample: 24000 / 60000 --> loss: 1.9509\n",
      "sample: 26000 / 60000 --> loss: 1.9407\n",
      "sample: 28000 / 60000 --> loss: 4.2670\n",
      "sample: 30000 / 60000 --> loss: 7.2685\n",
      "sample: 32000 / 60000 --> loss: 3.4379\n",
      "sample: 34000 / 60000 --> loss: 6.5329\n",
      "sample: 36000 / 60000 --> loss: 4.4610\n",
      "sample: 38000 / 60000 --> loss: 2.5941\n",
      "sample: 40000 / 60000 --> loss: 4.9893\n",
      "sample: 42000 / 60000 --> loss: 32.2192\n",
      "sample: 44000 / 60000 --> loss: 4.3839\n",
      "sample: 46000 / 60000 --> loss: 3.0679\n",
      "sample: 48000 / 60000 --> loss: 2.8363\n",
      "sample: 50000 / 60000 --> loss: 4.5116\n",
      "sample: 52000 / 60000 --> loss: 8.3850\n",
      "sample: 54000 / 60000 --> loss: 20.4523\n",
      "sample: 56000 / 60000 --> loss: 5.4942\n",
      "sample: 58000 / 60000 --> loss: 3.4500\n",
      "sample: 60000 / 60000 --> loss: 10.7057\n",
      "epoch: 06 / 25\n",
      "sample: 02000 / 60000 --> loss: 14.0700\n",
      "sample: 04000 / 60000 --> loss: 7.5918\n",
      "sample: 06000 / 60000 --> loss: 3.2724\n",
      "sample: 08000 / 60000 --> loss: 2.0760\n",
      "sample: 10000 / 60000 --> loss: 3.3939\n",
      "sample: 12000 / 60000 --> loss: 18.8288\n",
      "sample: 14000 / 60000 --> loss: 3.1767\n",
      "sample: 16000 / 60000 --> loss: 7.3630\n",
      "sample: 18000 / 60000 --> loss: 4.8005\n",
      "sample: 20000 / 60000 --> loss: 3.1952\n",
      "sample: 22000 / 60000 --> loss: 1.5752\n",
      "sample: 24000 / 60000 --> loss: 1.6650\n",
      "sample: 26000 / 60000 --> loss: 1.4448\n",
      "sample: 28000 / 60000 --> loss: 4.0814\n",
      "sample: 30000 / 60000 --> loss: 4.3242\n",
      "sample: 32000 / 60000 --> loss: 2.2451\n",
      "sample: 34000 / 60000 --> loss: 3.5485\n",
      "sample: 36000 / 60000 --> loss: 4.0690\n",
      "sample: 38000 / 60000 --> loss: 2.6885\n",
      "sample: 40000 / 60000 --> loss: 6.7191\n",
      "sample: 42000 / 60000 --> loss: 33.2838\n",
      "sample: 44000 / 60000 --> loss: 3.7209\n",
      "sample: 46000 / 60000 --> loss: 3.1755\n",
      "sample: 48000 / 60000 --> loss: 2.9451\n",
      "sample: 50000 / 60000 --> loss: 3.7347\n",
      "sample: 52000 / 60000 --> loss: 7.9137\n",
      "sample: 54000 / 60000 --> loss: 19.7938\n",
      "sample: 56000 / 60000 --> loss: 5.2670\n",
      "sample: 58000 / 60000 --> loss: 2.9359\n",
      "sample: 60000 / 60000 --> loss: 9.7354\n",
      "epoch: 07 / 25\n",
      "sample: 02000 / 60000 --> loss: 14.6875\n",
      "sample: 04000 / 60000 --> loss: 2.5425\n",
      "sample: 06000 / 60000 --> loss: 2.4116\n",
      "sample: 08000 / 60000 --> loss: 1.5261\n",
      "sample: 10000 / 60000 --> loss: 2.9268\n",
      "sample: 12000 / 60000 --> loss: 15.9897\n",
      "sample: 14000 / 60000 --> loss: 2.2865\n",
      "sample: 16000 / 60000 --> loss: 5.0486\n",
      "sample: 18000 / 60000 --> loss: 4.0185\n",
      "sample: 20000 / 60000 --> loss: 2.3694\n",
      "sample: 22000 / 60000 --> loss: 0.7055\n",
      "sample: 24000 / 60000 --> loss: 1.5877\n",
      "sample: 26000 / 60000 --> loss: 0.4093\n",
      "sample: 28000 / 60000 --> loss: 4.2400\n",
      "sample: 30000 / 60000 --> loss: 3.0060\n",
      "sample: 32000 / 60000 --> loss: 2.2706\n",
      "sample: 34000 / 60000 --> loss: 2.3076\n",
      "sample: 36000 / 60000 --> loss: 2.6029\n",
      "sample: 38000 / 60000 --> loss: 2.2154\n",
      "sample: 40000 / 60000 --> loss: 6.0261\n",
      "sample: 42000 / 60000 --> loss: 25.6266\n",
      "sample: 44000 / 60000 --> loss: 2.2467\n",
      "sample: 46000 / 60000 --> loss: 1.3004\n",
      "sample: 48000 / 60000 --> loss: 2.0904\n",
      "sample: 50000 / 60000 --> loss: 2.6273\n",
      "sample: 52000 / 60000 --> loss: 3.8092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: 54000 / 60000 --> loss: 25.8932\n",
      "sample: 56000 / 60000 --> loss: 3.6168\n",
      "sample: 58000 / 60000 --> loss: 2.8664\n",
      "sample: 60000 / 60000 --> loss: 7.4816\n",
      "epoch: 08 / 25\n",
      "sample: 02000 / 60000 --> loss: 15.1177\n",
      "sample: 04000 / 60000 --> loss: 2.4527\n",
      "sample: 06000 / 60000 --> loss: 2.2669\n",
      "sample: 08000 / 60000 --> loss: 1.1754\n",
      "sample: 10000 / 60000 --> loss: 2.2673\n",
      "sample: 12000 / 60000 --> loss: 14.4091\n",
      "sample: 14000 / 60000 --> loss: 1.9182\n",
      "sample: 16000 / 60000 --> loss: 5.2432\n",
      "sample: 18000 / 60000 --> loss: 3.6879\n",
      "sample: 20000 / 60000 --> loss: 2.1331\n",
      "sample: 22000 / 60000 --> loss: 0.6832\n",
      "sample: 24000 / 60000 --> loss: 1.6626\n",
      "sample: 26000 / 60000 --> loss: 0.4316\n",
      "sample: 28000 / 60000 --> loss: 4.0555\n",
      "sample: 30000 / 60000 --> loss: 2.9162\n",
      "sample: 32000 / 60000 --> loss: 2.1647\n",
      "sample: 34000 / 60000 --> loss: 1.7129\n",
      "sample: 36000 / 60000 --> loss: 2.7208\n",
      "sample: 38000 / 60000 --> loss: 1.9080\n",
      "sample: 40000 / 60000 --> loss: 5.6973\n",
      "sample: 42000 / 60000 --> loss: 24.9092\n",
      "sample: 44000 / 60000 --> loss: 2.0822\n",
      "sample: 46000 / 60000 --> loss: 1.2674\n",
      "sample: 48000 / 60000 --> loss: 1.9765\n",
      "sample: 50000 / 60000 --> loss: 2.8495\n",
      "sample: 52000 / 60000 --> loss: 3.3433\n",
      "sample: 54000 / 60000 --> loss: 26.9983\n",
      "sample: 56000 / 60000 --> loss: 3.2244\n",
      "sample: 58000 / 60000 --> loss: 2.8233\n",
      "sample: 60000 / 60000 --> loss: 6.8403\n",
      "epoch: 09 / 25\n",
      "sample: 02000 / 60000 --> loss: 14.7876\n",
      "sample: 04000 / 60000 --> loss: 2.4326\n",
      "sample: 06000 / 60000 --> loss: 2.0785\n",
      "sample: 08000 / 60000 --> loss: 1.0366\n",
      "sample: 10000 / 60000 --> loss: 2.1346\n",
      "sample: 12000 / 60000 --> loss: 14.0832\n",
      "sample: 14000 / 60000 --> loss: 1.7504\n",
      "sample: 16000 / 60000 --> loss: 5.3602\n",
      "sample: 18000 / 60000 --> loss: 3.5949\n",
      "sample: 20000 / 60000 --> loss: 1.8884\n",
      "sample: 22000 / 60000 --> loss: 0.6680\n",
      "sample: 24000 / 60000 --> loss: 1.7562\n",
      "sample: 26000 / 60000 --> loss: 0.4333\n",
      "sample: 28000 / 60000 --> loss: 3.9237\n",
      "sample: 30000 / 60000 --> loss: 2.7533\n",
      "sample: 32000 / 60000 --> loss: 2.0900\n",
      "sample: 34000 / 60000 --> loss: 1.3660\n",
      "sample: 36000 / 60000 --> loss: 2.9012\n",
      "sample: 38000 / 60000 --> loss: 1.6827\n",
      "sample: 40000 / 60000 --> loss: 5.4136\n",
      "sample: 42000 / 60000 --> loss: 23.0619\n",
      "sample: 44000 / 60000 --> loss: 1.8839\n",
      "sample: 46000 / 60000 --> loss: 1.2028\n",
      "sample: 48000 / 60000 --> loss: 1.8788\n",
      "sample: 50000 / 60000 --> loss: 3.1280\n",
      "sample: 52000 / 60000 --> loss: 2.9417\n",
      "sample: 54000 / 60000 --> loss: 27.8789\n",
      "sample: 56000 / 60000 --> loss: 2.9524\n",
      "sample: 58000 / 60000 --> loss: 2.7981\n",
      "sample: 60000 / 60000 --> loss: 6.4488\n",
      "epoch: 10 / 25\n",
      "sample: 02000 / 60000 --> loss: 14.2017\n",
      "sample: 04000 / 60000 --> loss: 2.3512\n",
      "sample: 06000 / 60000 --> loss: 1.8761\n",
      "sample: 08000 / 60000 --> loss: 1.0009\n",
      "sample: 10000 / 60000 --> loss: 2.1340\n",
      "sample: 12000 / 60000 --> loss: 14.0291\n",
      "sample: 14000 / 60000 --> loss: 1.6467\n",
      "sample: 16000 / 60000 --> loss: 5.3838\n",
      "sample: 18000 / 60000 --> loss: 3.5046\n",
      "sample: 20000 / 60000 --> loss: 1.7108\n",
      "sample: 22000 / 60000 --> loss: 0.6747\n",
      "sample: 24000 / 60000 --> loss: 1.8593\n",
      "sample: 26000 / 60000 --> loss: 0.4346\n",
      "sample: 28000 / 60000 --> loss: 3.7852\n",
      "sample: 30000 / 60000 --> loss: 2.5348\n",
      "sample: 32000 / 60000 --> loss: 2.0614\n",
      "sample: 34000 / 60000 --> loss: 1.1460\n",
      "sample: 36000 / 60000 --> loss: 3.1027\n",
      "sample: 38000 / 60000 --> loss: 1.5291\n",
      "sample: 40000 / 60000 --> loss: 5.0783\n",
      "sample: 42000 / 60000 --> loss: 20.8657\n",
      "sample: 44000 / 60000 --> loss: 1.6949\n",
      "sample: 46000 / 60000 --> loss: 1.1245\n",
      "sample: 48000 / 60000 --> loss: 1.8264\n",
      "sample: 50000 / 60000 --> loss: 3.3463\n",
      "sample: 52000 / 60000 --> loss: 2.7407\n",
      "sample: 54000 / 60000 --> loss: 28.6421\n",
      "sample: 56000 / 60000 --> loss: 2.7748\n",
      "sample: 58000 / 60000 --> loss: 2.7522\n",
      "sample: 60000 / 60000 --> loss: 6.1146\n",
      "epoch: 11 / 25\n",
      "sample: 02000 / 60000 --> loss: 13.2959\n",
      "sample: 04000 / 60000 --> loss: 2.2268\n",
      "sample: 06000 / 60000 --> loss: 1.7083\n",
      "sample: 08000 / 60000 --> loss: 1.0317\n",
      "sample: 10000 / 60000 --> loss: 2.1426\n",
      "sample: 12000 / 60000 --> loss: 14.1454\n",
      "sample: 14000 / 60000 --> loss: 1.6163\n",
      "sample: 16000 / 60000 --> loss: 5.3678\n",
      "sample: 18000 / 60000 --> loss: 3.3654\n",
      "sample: 20000 / 60000 --> loss: 1.5779\n",
      "sample: 22000 / 60000 --> loss: 0.6947\n",
      "sample: 24000 / 60000 --> loss: 1.9891\n",
      "sample: 26000 / 60000 --> loss: 0.4471\n",
      "sample: 28000 / 60000 --> loss: 3.5750\n",
      "sample: 30000 / 60000 --> loss: 2.3015\n",
      "sample: 32000 / 60000 --> loss: 2.0545\n",
      "sample: 34000 / 60000 --> loss: 0.9837\n",
      "sample: 36000 / 60000 --> loss: 3.2941\n",
      "sample: 38000 / 60000 --> loss: 1.4269\n",
      "sample: 40000 / 60000 --> loss: 4.7015\n",
      "sample: 42000 / 60000 --> loss: 19.0177\n",
      "sample: 44000 / 60000 --> loss: 1.5299\n",
      "sample: 46000 / 60000 --> loss: 1.0351\n",
      "sample: 48000 / 60000 --> loss: 1.8355\n",
      "sample: 50000 / 60000 --> loss: 3.4203\n",
      "sample: 52000 / 60000 --> loss: 2.6851\n",
      "sample: 54000 / 60000 --> loss: 29.1886\n",
      "sample: 56000 / 60000 --> loss: 2.6580\n",
      "sample: 58000 / 60000 --> loss: 2.7356\n",
      "sample: 60000 / 60000 --> loss: 5.8108\n",
      "epoch: 12 / 25\n",
      "sample: 02000 / 60000 --> loss: 10.1499\n",
      "sample: 04000 / 60000 --> loss: 1.2464\n",
      "sample: 06000 / 60000 --> loss: 1.3845\n",
      "sample: 08000 / 60000 --> loss: 1.1019\n",
      "sample: 10000 / 60000 --> loss: 1.8889\n",
      "sample: 12000 / 60000 --> loss: 16.6855\n",
      "sample: 14000 / 60000 --> loss: 1.4873\n",
      "sample: 16000 / 60000 --> loss: 4.5948\n",
      "sample: 18000 / 60000 --> loss: 3.1498\n",
      "sample: 20000 / 60000 --> loss: 1.0978\n",
      "sample: 22000 / 60000 --> loss: 0.4486\n",
      "sample: 24000 / 60000 --> loss: 1.7613\n",
      "sample: 26000 / 60000 --> loss: 0.3986\n",
      "sample: 28000 / 60000 --> loss: 3.5801\n",
      "sample: 30000 / 60000 --> loss: 1.8549\n",
      "sample: 32000 / 60000 --> loss: 2.1692\n",
      "sample: 34000 / 60000 --> loss: 1.0825\n",
      "sample: 36000 / 60000 --> loss: 2.7947\n",
      "sample: 38000 / 60000 --> loss: 1.5333\n",
      "sample: 40000 / 60000 --> loss: 5.1551\n",
      "sample: 42000 / 60000 --> loss: 13.6948\n",
      "sample: 44000 / 60000 --> loss: 1.1544\n",
      "sample: 46000 / 60000 --> loss: 0.2934\n",
      "sample: 48000 / 60000 --> loss: 1.6551\n",
      "sample: 50000 / 60000 --> loss: 2.9679\n",
      "sample: 52000 / 60000 --> loss: 1.9759\n",
      "sample: 54000 / 60000 --> loss: 35.6397\n",
      "sample: 56000 / 60000 --> loss: 1.3283\n",
      "sample: 58000 / 60000 --> loss: 2.5563\n",
      "sample: 60000 / 60000 --> loss: 4.0757\n",
      "epoch: 13 / 25\n",
      "sample: 02000 / 60000 --> loss: 10.4828\n",
      "sample: 04000 / 60000 --> loss: 1.2298\n",
      "sample: 06000 / 60000 --> loss: 1.1964\n",
      "sample: 08000 / 60000 --> loss: 1.0095\n",
      "sample: 10000 / 60000 --> loss: 1.7026\n",
      "sample: 12000 / 60000 --> loss: 16.2088\n",
      "sample: 14000 / 60000 --> loss: 1.4933\n",
      "sample: 16000 / 60000 --> loss: 4.7048\n",
      "sample: 18000 / 60000 --> loss: 3.2036\n",
      "sample: 20000 / 60000 --> loss: 1.1320\n",
      "sample: 22000 / 60000 --> loss: 0.4812\n",
      "sample: 24000 / 60000 --> loss: 1.7678\n",
      "sample: 26000 / 60000 --> loss: 0.4029\n",
      "sample: 28000 / 60000 --> loss: 3.3227\n",
      "sample: 30000 / 60000 --> loss: 1.7989\n",
      "sample: 32000 / 60000 --> loss: 2.1814\n",
      "sample: 34000 / 60000 --> loss: 0.8752\n",
      "sample: 36000 / 60000 --> loss: 2.8164\n",
      "sample: 38000 / 60000 --> loss: 1.5723\n",
      "sample: 40000 / 60000 --> loss: 4.8261\n",
      "sample: 42000 / 60000 --> loss: 13.4058\n",
      "sample: 44000 / 60000 --> loss: 1.1390\n",
      "sample: 46000 / 60000 --> loss: 0.2655\n",
      "sample: 48000 / 60000 --> loss: 1.6524\n",
      "sample: 50000 / 60000 --> loss: 2.9748\n",
      "sample: 52000 / 60000 --> loss: 1.9968\n",
      "sample: 54000 / 60000 --> loss: 35.6784\n",
      "sample: 56000 / 60000 --> loss: 1.2876\n",
      "sample: 58000 / 60000 --> loss: 2.4641\n",
      "sample: 60000 / 60000 --> loss: 3.9665\n",
      "epoch: 14 / 25\n",
      "sample: 02000 / 60000 --> loss: 10.0818\n",
      "sample: 04000 / 60000 --> loss: 1.2499\n",
      "sample: 06000 / 60000 --> loss: 1.1513\n",
      "sample: 08000 / 60000 --> loss: 0.9764\n",
      "sample: 10000 / 60000 --> loss: 1.6368\n",
      "sample: 12000 / 60000 --> loss: 16.3979\n",
      "sample: 14000 / 60000 --> loss: 1.5024\n",
      "sample: 16000 / 60000 --> loss: 4.8071\n",
      "sample: 18000 / 60000 --> loss: 3.2339\n",
      "sample: 20000 / 60000 --> loss: 1.1750\n",
      "sample: 22000 / 60000 --> loss: 0.4999\n",
      "sample: 24000 / 60000 --> loss: 1.8064\n",
      "sample: 26000 / 60000 --> loss: 0.4023\n",
      "sample: 28000 / 60000 --> loss: 3.0903\n",
      "sample: 30000 / 60000 --> loss: 1.7231\n",
      "sample: 32000 / 60000 --> loss: 2.1867\n",
      "sample: 34000 / 60000 --> loss: 0.7569\n",
      "sample: 36000 / 60000 --> loss: 2.8174\n",
      "sample: 38000 / 60000 --> loss: 1.6098\n",
      "sample: 40000 / 60000 --> loss: 4.5646\n",
      "sample: 42000 / 60000 --> loss: 13.1476\n",
      "sample: 44000 / 60000 --> loss: 1.1153\n",
      "sample: 46000 / 60000 --> loss: 0.2584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: 48000 / 60000 --> loss: 1.6507\n",
      "sample: 50000 / 60000 --> loss: 2.9627\n",
      "sample: 52000 / 60000 --> loss: 2.0140\n",
      "sample: 54000 / 60000 --> loss: 35.3629\n",
      "sample: 56000 / 60000 --> loss: 1.2705\n",
      "sample: 58000 / 60000 --> loss: 2.4020\n",
      "sample: 60000 / 60000 --> loss: 3.8847\n",
      "epoch: 15 / 25\n",
      "sample: 02000 / 60000 --> loss: 9.7685\n",
      "sample: 04000 / 60000 --> loss: 1.2645\n",
      "sample: 06000 / 60000 --> loss: 1.1254\n",
      "sample: 08000 / 60000 --> loss: 0.9563\n",
      "sample: 10000 / 60000 --> loss: 1.5871\n",
      "sample: 12000 / 60000 --> loss: 16.4277\n",
      "sample: 14000 / 60000 --> loss: 1.5058\n",
      "sample: 16000 / 60000 --> loss: 4.8920\n",
      "sample: 18000 / 60000 --> loss: 3.2319\n",
      "sample: 20000 / 60000 --> loss: 1.2270\n",
      "sample: 22000 / 60000 --> loss: 0.5148\n",
      "sample: 24000 / 60000 --> loss: 1.8496\n",
      "sample: 26000 / 60000 --> loss: 0.4068\n",
      "sample: 28000 / 60000 --> loss: 2.8793\n",
      "sample: 30000 / 60000 --> loss: 1.6421\n",
      "sample: 32000 / 60000 --> loss: 2.1885\n",
      "sample: 34000 / 60000 --> loss: 0.6786\n",
      "sample: 36000 / 60000 --> loss: 2.7817\n",
      "sample: 38000 / 60000 --> loss: 1.6230\n",
      "sample: 40000 / 60000 --> loss: 4.3367\n",
      "sample: 42000 / 60000 --> loss: 12.9348\n",
      "sample: 44000 / 60000 --> loss: 1.0937\n",
      "sample: 46000 / 60000 --> loss: 0.2529\n",
      "sample: 48000 / 60000 --> loss: 1.6423\n",
      "sample: 50000 / 60000 --> loss: 2.9451\n",
      "sample: 52000 / 60000 --> loss: 2.0223\n",
      "sample: 54000 / 60000 --> loss: 34.8746\n",
      "sample: 56000 / 60000 --> loss: 1.2628\n",
      "sample: 58000 / 60000 --> loss: 2.3614\n",
      "sample: 60000 / 60000 --> loss: 3.8230\n",
      "epoch: 16 / 25\n",
      "sample: 02000 / 60000 --> loss: 9.5278\n",
      "sample: 04000 / 60000 --> loss: 1.2843\n",
      "sample: 06000 / 60000 --> loss: 1.1089\n",
      "sample: 08000 / 60000 --> loss: 0.9411\n",
      "sample: 10000 / 60000 --> loss: 1.5543\n",
      "sample: 12000 / 60000 --> loss: 16.3522\n",
      "sample: 14000 / 60000 --> loss: 1.5042\n",
      "sample: 16000 / 60000 --> loss: 4.9668\n",
      "sample: 18000 / 60000 --> loss: 3.2312\n",
      "sample: 20000 / 60000 --> loss: 1.2932\n",
      "sample: 22000 / 60000 --> loss: 0.5241\n",
      "sample: 24000 / 60000 --> loss: 1.8952\n",
      "sample: 26000 / 60000 --> loss: 0.4165\n",
      "sample: 28000 / 60000 --> loss: 2.6699\n",
      "sample: 30000 / 60000 --> loss: 1.5737\n",
      "sample: 32000 / 60000 --> loss: 2.1795\n",
      "sample: 34000 / 60000 --> loss: 0.6217\n",
      "sample: 36000 / 60000 --> loss: 2.7256\n",
      "sample: 38000 / 60000 --> loss: 1.6174\n",
      "sample: 40000 / 60000 --> loss: 4.1169\n",
      "sample: 42000 / 60000 --> loss: 12.7350\n",
      "sample: 44000 / 60000 --> loss: 1.0794\n",
      "sample: 46000 / 60000 --> loss: 0.2526\n",
      "sample: 48000 / 60000 --> loss: 1.6381\n",
      "sample: 50000 / 60000 --> loss: 2.8982\n",
      "sample: 52000 / 60000 --> loss: 2.0260\n",
      "sample: 54000 / 60000 --> loss: 34.2917\n",
      "sample: 56000 / 60000 --> loss: 1.2588\n",
      "sample: 58000 / 60000 --> loss: 2.3392\n",
      "sample: 60000 / 60000 --> loss: 3.7824\n",
      "epoch: 17 / 25\n",
      "sample: 02000 / 60000 --> loss: 8.8860\n",
      "sample: 04000 / 60000 --> loss: 1.3688\n",
      "sample: 06000 / 60000 --> loss: 1.2631\n",
      "sample: 08000 / 60000 --> loss: 1.1169\n",
      "sample: 10000 / 60000 --> loss: 1.1767\n",
      "sample: 12000 / 60000 --> loss: 18.5869\n",
      "sample: 14000 / 60000 --> loss: 1.6683\n",
      "sample: 16000 / 60000 --> loss: 4.7243\n",
      "sample: 18000 / 60000 --> loss: 3.1882\n",
      "sample: 20000 / 60000 --> loss: 1.3478\n",
      "sample: 22000 / 60000 --> loss: 0.5365\n",
      "sample: 24000 / 60000 --> loss: 1.4823\n",
      "sample: 26000 / 60000 --> loss: 0.6538\n",
      "sample: 28000 / 60000 --> loss: 2.5656\n",
      "sample: 30000 / 60000 --> loss: 1.4387\n",
      "sample: 32000 / 60000 --> loss: 1.9960\n",
      "sample: 34000 / 60000 --> loss: 0.9241\n",
      "sample: 36000 / 60000 --> loss: 2.5499\n",
      "sample: 38000 / 60000 --> loss: 1.8539\n",
      "sample: 40000 / 60000 --> loss: 4.8387\n",
      "sample: 42000 / 60000 --> loss: 11.3514\n",
      "sample: 44000 / 60000 --> loss: 1.1783\n",
      "sample: 46000 / 60000 --> loss: 0.1496\n",
      "sample: 48000 / 60000 --> loss: 1.6787\n",
      "sample: 50000 / 60000 --> loss: 3.1302\n",
      "sample: 52000 / 60000 --> loss: 1.8586\n",
      "sample: 54000 / 60000 --> loss: 37.7806\n",
      "sample: 56000 / 60000 --> loss: 0.7544\n",
      "sample: 58000 / 60000 --> loss: 2.1042\n",
      "sample: 60000 / 60000 --> loss: 3.0872\n",
      "epoch: 18 / 25\n",
      "sample: 02000 / 60000 --> loss: 8.9161\n",
      "sample: 04000 / 60000 --> loss: 1.3628\n",
      "sample: 06000 / 60000 --> loss: 1.2239\n",
      "sample: 08000 / 60000 --> loss: 1.0571\n",
      "sample: 10000 / 60000 --> loss: 1.1851\n",
      "sample: 12000 / 60000 --> loss: 17.7783\n",
      "sample: 14000 / 60000 --> loss: 1.6891\n",
      "sample: 16000 / 60000 --> loss: 4.7565\n",
      "sample: 18000 / 60000 --> loss: 3.2245\n",
      "sample: 20000 / 60000 --> loss: 1.4303\n",
      "sample: 22000 / 60000 --> loss: 0.5525\n",
      "sample: 24000 / 60000 --> loss: 1.4868\n",
      "sample: 26000 / 60000 --> loss: 0.6762\n",
      "sample: 28000 / 60000 --> loss: 2.4104\n",
      "sample: 30000 / 60000 --> loss: 1.4108\n",
      "sample: 32000 / 60000 --> loss: 1.9542\n",
      "sample: 34000 / 60000 --> loss: 0.8443\n",
      "sample: 36000 / 60000 --> loss: 2.4454\n",
      "sample: 38000 / 60000 --> loss: 1.8702\n",
      "sample: 40000 / 60000 --> loss: 4.6947\n",
      "sample: 42000 / 60000 --> loss: 11.1727\n",
      "sample: 44000 / 60000 --> loss: 1.1965\n",
      "sample: 46000 / 60000 --> loss: 0.1472\n",
      "sample: 48000 / 60000 --> loss: 1.6909\n",
      "sample: 50000 / 60000 --> loss: 3.0879\n",
      "sample: 52000 / 60000 --> loss: 1.8506\n",
      "sample: 54000 / 60000 --> loss: 37.7408\n",
      "sample: 56000 / 60000 --> loss: 0.7350\n",
      "sample: 58000 / 60000 --> loss: 2.0830\n",
      "sample: 60000 / 60000 --> loss: 3.0857\n",
      "epoch: 19 / 25\n",
      "sample: 02000 / 60000 --> loss: 8.8011\n",
      "sample: 04000 / 60000 --> loss: 1.3527\n",
      "sample: 06000 / 60000 --> loss: 1.2003\n",
      "sample: 08000 / 60000 --> loss: 1.0312\n",
      "sample: 10000 / 60000 --> loss: 1.1869\n",
      "sample: 12000 / 60000 --> loss: 17.2766\n",
      "sample: 14000 / 60000 --> loss: 1.6619\n",
      "sample: 16000 / 60000 --> loss: 4.8054\n",
      "sample: 18000 / 60000 --> loss: 3.2351\n",
      "sample: 20000 / 60000 --> loss: 1.4833\n",
      "sample: 22000 / 60000 --> loss: 0.5528\n",
      "sample: 24000 / 60000 --> loss: 1.4954\n",
      "sample: 26000 / 60000 --> loss: 0.6914\n",
      "sample: 28000 / 60000 --> loss: 2.2884\n",
      "sample: 30000 / 60000 --> loss: 1.3941\n",
      "sample: 32000 / 60000 --> loss: 1.9232\n",
      "sample: 34000 / 60000 --> loss: 0.7846\n",
      "sample: 36000 / 60000 --> loss: 2.3594\n",
      "sample: 38000 / 60000 --> loss: 1.8626\n",
      "sample: 40000 / 60000 --> loss: 4.5579\n",
      "sample: 42000 / 60000 --> loss: 10.9984\n",
      "sample: 44000 / 60000 --> loss: 1.2112\n",
      "sample: 46000 / 60000 --> loss: 0.1464\n",
      "sample: 48000 / 60000 --> loss: 1.7053\n",
      "sample: 50000 / 60000 --> loss: 3.0445\n",
      "sample: 52000 / 60000 --> loss: 1.8522\n",
      "sample: 54000 / 60000 --> loss: 37.5636\n",
      "sample: 56000 / 60000 --> loss: 0.7203\n",
      "sample: 58000 / 60000 --> loss: 2.0702\n",
      "sample: 60000 / 60000 --> loss: 3.0873\n",
      "epoch: 20 / 25\n",
      "sample: 02000 / 60000 --> loss: 8.7047\n",
      "sample: 04000 / 60000 --> loss: 1.3321\n",
      "sample: 06000 / 60000 --> loss: 1.1775\n",
      "sample: 08000 / 60000 --> loss: 1.0152\n",
      "sample: 10000 / 60000 --> loss: 1.1880\n",
      "sample: 12000 / 60000 --> loss: 16.7187\n",
      "sample: 14000 / 60000 --> loss: 1.6461\n",
      "sample: 16000 / 60000 --> loss: 4.8333\n",
      "sample: 18000 / 60000 --> loss: 3.2256\n",
      "sample: 20000 / 60000 --> loss: 1.5360\n",
      "sample: 22000 / 60000 --> loss: 0.5497\n",
      "sample: 24000 / 60000 --> loss: 1.5108\n",
      "sample: 26000 / 60000 --> loss: 0.7115\n",
      "sample: 28000 / 60000 --> loss: 2.1710\n",
      "sample: 30000 / 60000 --> loss: 1.3669\n",
      "sample: 32000 / 60000 --> loss: 1.8957\n",
      "sample: 34000 / 60000 --> loss: 0.7378\n",
      "sample: 36000 / 60000 --> loss: 2.2910\n",
      "sample: 38000 / 60000 --> loss: 1.8390\n",
      "sample: 40000 / 60000 --> loss: 4.4238\n",
      "sample: 42000 / 60000 --> loss: 10.8041\n",
      "sample: 44000 / 60000 --> loss: 1.2210\n",
      "sample: 46000 / 60000 --> loss: 0.1430\n",
      "sample: 48000 / 60000 --> loss: 1.7179\n",
      "sample: 50000 / 60000 --> loss: 2.9999\n",
      "sample: 52000 / 60000 --> loss: 1.8616\n",
      "sample: 54000 / 60000 --> loss: 37.3181\n",
      "sample: 56000 / 60000 --> loss: 0.7072\n",
      "sample: 58000 / 60000 --> loss: 2.0658\n",
      "sample: 60000 / 60000 --> loss: 3.0914\n",
      "epoch: 21 / 25\n",
      "sample: 02000 / 60000 --> loss: 8.6158\n",
      "sample: 04000 / 60000 --> loss: 1.3030\n",
      "sample: 06000 / 60000 --> loss: 1.1524\n",
      "sample: 08000 / 60000 --> loss: 1.0018\n",
      "sample: 10000 / 60000 --> loss: 1.1978\n",
      "sample: 12000 / 60000 --> loss: 16.1244\n",
      "sample: 14000 / 60000 --> loss: 1.6310\n",
      "sample: 16000 / 60000 --> loss: 4.8345\n",
      "sample: 18000 / 60000 --> loss: 3.2171\n",
      "sample: 20000 / 60000 --> loss: 1.5825\n",
      "sample: 22000 / 60000 --> loss: 0.5422\n",
      "sample: 24000 / 60000 --> loss: 1.5281\n",
      "sample: 26000 / 60000 --> loss: 0.7361\n",
      "sample: 28000 / 60000 --> loss: 2.0609\n",
      "sample: 30000 / 60000 --> loss: 1.3429\n",
      "sample: 32000 / 60000 --> loss: 1.8718\n",
      "sample: 34000 / 60000 --> loss: 0.7005\n",
      "sample: 36000 / 60000 --> loss: 2.2536\n",
      "sample: 38000 / 60000 --> loss: 1.8032\n",
      "sample: 40000 / 60000 --> loss: 4.2774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: 42000 / 60000 --> loss: 10.6000\n",
      "sample: 44000 / 60000 --> loss: 1.2311\n",
      "sample: 46000 / 60000 --> loss: 0.1369\n",
      "sample: 48000 / 60000 --> loss: 1.7251\n",
      "sample: 50000 / 60000 --> loss: 2.9545\n",
      "sample: 52000 / 60000 --> loss: 1.8668\n",
      "sample: 54000 / 60000 --> loss: 36.9623\n",
      "sample: 56000 / 60000 --> loss: 0.6935\n",
      "sample: 58000 / 60000 --> loss: 2.0717\n",
      "sample: 60000 / 60000 --> loss: 3.0899\n",
      "epoch: 22 / 25\n",
      "sample: 02000 / 60000 --> loss: 8.5581\n",
      "sample: 04000 / 60000 --> loss: 1.2594\n",
      "sample: 06000 / 60000 --> loss: 1.1221\n",
      "sample: 08000 / 60000 --> loss: 0.9939\n",
      "sample: 10000 / 60000 --> loss: 1.2235\n",
      "sample: 12000 / 60000 --> loss: 15.5459\n",
      "sample: 14000 / 60000 --> loss: 1.6205\n",
      "sample: 16000 / 60000 --> loss: 4.8120\n",
      "sample: 18000 / 60000 --> loss: 3.2005\n",
      "sample: 20000 / 60000 --> loss: 1.6239\n",
      "sample: 22000 / 60000 --> loss: 0.5317\n",
      "sample: 24000 / 60000 --> loss: 1.5382\n",
      "sample: 26000 / 60000 --> loss: 0.7682\n",
      "sample: 28000 / 60000 --> loss: 1.9476\n",
      "sample: 30000 / 60000 --> loss: 1.3256\n",
      "sample: 32000 / 60000 --> loss: 1.8574\n",
      "sample: 34000 / 60000 --> loss: 0.6733\n",
      "sample: 36000 / 60000 --> loss: 2.2238\n",
      "sample: 38000 / 60000 --> loss: 1.7541\n",
      "sample: 40000 / 60000 --> loss: 4.1407\n",
      "sample: 42000 / 60000 --> loss: 10.4037\n",
      "sample: 44000 / 60000 --> loss: 1.2358\n",
      "sample: 46000 / 60000 --> loss: 0.1318\n",
      "sample: 48000 / 60000 --> loss: 1.7324\n",
      "sample: 50000 / 60000 --> loss: 2.9081\n",
      "sample: 52000 / 60000 --> loss: 1.8588\n",
      "sample: 54000 / 60000 --> loss: 36.5495\n",
      "sample: 56000 / 60000 --> loss: 0.6807\n",
      "sample: 58000 / 60000 --> loss: 2.0849\n",
      "sample: 60000 / 60000 --> loss: 3.0905\n",
      "epoch: 23 / 25\n",
      "sample: 02000 / 60000 --> loss: 8.5362\n",
      "sample: 04000 / 60000 --> loss: 1.2006\n",
      "sample: 06000 / 60000 --> loss: 1.0856\n",
      "sample: 08000 / 60000 --> loss: 0.9864\n",
      "sample: 10000 / 60000 --> loss: 1.2567\n",
      "sample: 12000 / 60000 --> loss: 14.9723\n",
      "sample: 14000 / 60000 --> loss: 1.6217\n",
      "sample: 16000 / 60000 --> loss: 4.7684\n",
      "sample: 18000 / 60000 --> loss: 3.1910\n",
      "sample: 20000 / 60000 --> loss: 1.6568\n",
      "sample: 22000 / 60000 --> loss: 0.5223\n",
      "sample: 24000 / 60000 --> loss: 1.5465\n",
      "sample: 26000 / 60000 --> loss: 0.8012\n",
      "sample: 28000 / 60000 --> loss: 1.8270\n",
      "sample: 30000 / 60000 --> loss: 1.3144\n",
      "sample: 32000 / 60000 --> loss: 1.8493\n",
      "sample: 34000 / 60000 --> loss: 0.6563\n",
      "sample: 36000 / 60000 --> loss: 2.2191\n",
      "sample: 38000 / 60000 --> loss: 1.6973\n",
      "sample: 40000 / 60000 --> loss: 4.0024\n",
      "sample: 42000 / 60000 --> loss: 10.2401\n",
      "sample: 44000 / 60000 --> loss: 1.2380\n",
      "sample: 46000 / 60000 --> loss: 0.1241\n",
      "sample: 48000 / 60000 --> loss: 1.7286\n",
      "sample: 50000 / 60000 --> loss: 2.8679\n",
      "sample: 52000 / 60000 --> loss: 1.8446\n",
      "sample: 54000 / 60000 --> loss: 36.0418\n",
      "sample: 56000 / 60000 --> loss: 0.6719\n",
      "sample: 58000 / 60000 --> loss: 2.0935\n",
      "sample: 60000 / 60000 --> loss: 3.0792\n",
      "epoch: 24 / 25\n",
      "sample: 02000 / 60000 --> loss: 8.5434\n",
      "sample: 04000 / 60000 --> loss: 1.1254\n",
      "sample: 06000 / 60000 --> loss: 1.0400\n",
      "sample: 08000 / 60000 --> loss: 0.9834\n",
      "sample: 10000 / 60000 --> loss: 1.2833\n",
      "sample: 12000 / 60000 --> loss: 14.4392\n",
      "sample: 14000 / 60000 --> loss: 1.6285\n",
      "sample: 16000 / 60000 --> loss: 4.7123\n",
      "sample: 18000 / 60000 --> loss: 3.1918\n",
      "sample: 20000 / 60000 --> loss: 1.6776\n",
      "sample: 22000 / 60000 --> loss: 0.5133\n",
      "sample: 24000 / 60000 --> loss: 1.5492\n",
      "sample: 26000 / 60000 --> loss: 0.8257\n",
      "sample: 28000 / 60000 --> loss: 1.7096\n",
      "sample: 30000 / 60000 --> loss: 1.3011\n",
      "sample: 32000 / 60000 --> loss: 1.8426\n",
      "sample: 34000 / 60000 --> loss: 0.6515\n",
      "sample: 36000 / 60000 --> loss: 2.2336\n",
      "sample: 38000 / 60000 --> loss: 1.6455\n",
      "sample: 40000 / 60000 --> loss: 3.8590\n",
      "sample: 42000 / 60000 --> loss: 10.1198\n",
      "sample: 44000 / 60000 --> loss: 1.2352\n",
      "sample: 46000 / 60000 --> loss: 0.1174\n",
      "sample: 48000 / 60000 --> loss: 1.7111\n",
      "sample: 50000 / 60000 --> loss: 2.8335\n",
      "sample: 52000 / 60000 --> loss: 1.8167\n",
      "sample: 54000 / 60000 --> loss: 35.4991\n",
      "sample: 56000 / 60000 --> loss: 0.6666\n",
      "sample: 58000 / 60000 --> loss: 2.0961\n",
      "sample: 60000 / 60000 --> loss: 3.0793\n",
      "epoch: 25 / 25\n",
      "sample: 02000 / 60000 --> loss: 8.5684\n",
      "sample: 04000 / 60000 --> loss: 1.0467\n",
      "sample: 06000 / 60000 --> loss: 0.9941\n",
      "sample: 08000 / 60000 --> loss: 0.9820\n",
      "sample: 10000 / 60000 --> loss: 1.3125\n",
      "sample: 12000 / 60000 --> loss: 14.0297\n",
      "sample: 14000 / 60000 --> loss: 1.6338\n",
      "sample: 16000 / 60000 --> loss: 4.6560\n",
      "sample: 18000 / 60000 --> loss: 3.1938\n",
      "sample: 20000 / 60000 --> loss: 1.6886\n",
      "sample: 22000 / 60000 --> loss: 0.5060\n",
      "sample: 24000 / 60000 --> loss: 1.5506\n",
      "sample: 26000 / 60000 --> loss: 0.8270\n",
      "sample: 28000 / 60000 --> loss: 1.5926\n",
      "sample: 30000 / 60000 --> loss: 1.2991\n",
      "sample: 32000 / 60000 --> loss: 1.8344\n",
      "sample: 34000 / 60000 --> loss: 0.6553\n",
      "sample: 36000 / 60000 --> loss: 2.2628\n",
      "sample: 38000 / 60000 --> loss: 1.5933\n",
      "sample: 40000 / 60000 --> loss: 3.7015\n",
      "sample: 42000 / 60000 --> loss: 10.0603\n",
      "sample: 44000 / 60000 --> loss: 1.2275\n",
      "sample: 46000 / 60000 --> loss: 0.1124\n",
      "sample: 48000 / 60000 --> loss: 1.6962\n",
      "sample: 50000 / 60000 --> loss: 2.7952\n",
      "sample: 52000 / 60000 --> loss: 1.7798\n",
      "sample: 54000 / 60000 --> loss: 34.9215\n",
      "sample: 56000 / 60000 --> loss: 0.6693\n",
      "sample: 58000 / 60000 --> loss: 2.0878\n",
      "sample: 60000 / 60000 --> loss: 3.0801\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    model = LeNet5().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 1.0e-3)\n",
    "    \n",
    "    scheduler = True\n",
    "    \n",
    "    epochs = 25\n",
    "    \n",
    "    train(epochs, model, optimizer, scheduler, loss_fn, mnistTrain, mnistTest)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(lossList)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(trainError)\n",
    "    plt.subplot(1, 3 ,3)\n",
    "    plt.plot(testError)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f360e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
